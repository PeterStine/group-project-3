{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import splinter\n",
    "from splinter import Browser\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0818bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.localtime()\n",
    "current_time_file = time.strftime(\"%Y-%m-%d-%H-%M-%S\", t)\n",
    "\n",
    "log = open(f\"../logs/scraping_log_{current_time_file}.txt\", \"a\")\n",
    "log_file = f\"../logs/scraping_log_{current_time_file}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create browser object\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Indeed\n",
    "\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", t)\n",
    "\n",
    "try:\n",
    "    print(f\"Opening Indeed in Chrome...{current_time}\", file=log)    \n",
    "    \n",
    "    url = \"https://www.indeed.com/\"\n",
    "    browser.visit(url)\n",
    "except:\n",
    "    print(f\"Something went wrong trying to open Indeed in Chrome...{current_time}\", file=log)\n",
    "    print(f\"Something went wrong trying to open Indeed in Chrome...{current_time}\")\n",
    "    \n",
    "    print(f\"Logs have been saved to {log_file}\", file=log)\n",
    "    print(f\"Logs have been saved to {log_file}\")\n",
    "    \n",
    "    log.close()\n",
    "    quit()\n",
    "\n",
    "# Give the browser time to respond\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2debf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the / from the end of the url\n",
    "url[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store results in\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174739a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all the posts on a single page\n",
    "def scrape_one_page(data, browser):\n",
    "    # Job Title\n",
    "    posts_in_page = browser.find_by_css(\".jobsearch-ResultsList\").find_by_css(\".job_seen_beacon\")\n",
    "        \n",
    "    for post in posts_in_page:\n",
    "        # Initialize 1D array to store this post's data\n",
    "        post_data = []\n",
    "            \n",
    "        # Grab job id\n",
    "        try:\n",
    "            job_id = post.find_by_css(\"a\").first[\"data-jk\"]\n",
    "            post_data.append(job_id)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "            \n",
    "        # Grab job title\n",
    "        try:\n",
    "            title = post.find_by_css(\"span\").first.text\n",
    "            post_data.append(title)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "            \n",
    "        # Grab company\n",
    "        try:\n",
    "            company = post.find_by_css(\".companyName\").first.text\n",
    "            post_data.append(company)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "            \n",
    "        # Grab location\n",
    "        try:\n",
    "            location = post.find_by_css(\".companyLocation\").first.text\n",
    "            post_data.append(location)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "            \n",
    "        # Grab job type (if available)\n",
    "        try:\n",
    "            job_type = post.find_by_css(\"[aria-label='Job type']\").first.find_by_xpath(\"..\").first.text\n",
    "            post_data.append(job_type)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "                \n",
    "        # Grab salary (if available)\n",
    "        try:\n",
    "            salary = post.find_by_css(\"[aria-label='Salary']\").first.find_by_xpath(\"..\").first.text\n",
    "            post_data.append(salary)\n",
    "        except:\n",
    "            try:\n",
    "                salary = post.find_by_css(\".estimated-salary\").first.find_by_css(\"span\").first.text\n",
    "                post_data.append(salary)\n",
    "            except:\n",
    "                post_data.append(None)\n",
    "                \n",
    "        # Record the time\n",
    "        t = time.localtime()\n",
    "        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", t)\n",
    "        post_data.append(current_time)\n",
    "            \n",
    "        # Grab job url\n",
    "        try:\n",
    "            job_url = post.find_by_css(\"a\").first[\"href\"]\n",
    "            post_data.append(job_url)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "        \n",
    "        data.append(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all the posts on all pages\n",
    "def indeed_scraper(data, search_params, browser):\n",
    "    base_url = url[:-1]\n",
    "    # Find the what box & search the job title\n",
    "    what = browser.find_by_id(\"text-input-what\").first\n",
    "    what.fill(search_params[\"what\"])\n",
    "    \n",
    "    # Find the where box & search for the location\n",
    "    where = browser.find_by_id(\"text-input-where\").first\n",
    "    where.type(Keys.CONTROL + \"a\")\n",
    "    where.type(Keys.BACKSPACE)\n",
    "    where.fill(search_params[\"where\"])\n",
    "    \n",
    "    where.type(Keys.RETURN)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    date_posted = browser.find_by_text(\"Date posted\").first\n",
    "    date_posted.click()\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # This can be set to one of these options = [\"Last 24 Hours\", \"Last 3 days\", \"Last 7 days\", \"Last 14 days\"]\n",
    "    last_24_hours = browser.find_by_text(\"Last 24 hours\").first\n",
    "    last_24_hours.click()\n",
    "    \n",
    "    last_page = False\n",
    "    page_count = 0\n",
    "    \n",
    "    while not last_page:\n",
    "        try:  # This code runs for every page except the last page\n",
    "            next_page_button = browser.find_by_css(\"[aria-label='Next Page']\")\n",
    "            \n",
    "            page_count += 1\n",
    "            print(f\"Scraping page {page_count}...\", file=log)\n",
    "            print(f\"Scraping page {page_count}...\")\n",
    "        \n",
    "            post_data = scrape_one_page(data, browser)\n",
    "                            \n",
    "            print(\"Clicking to next page.\", file=log)\n",
    "            print(\"Clicking to next page.\")\n",
    "            next_page_button.click()\n",
    "                            \n",
    "        except:  # This code runs only for the last page\n",
    "            \n",
    "            page_count += 1\n",
    "            print(f\"Scraping final page of {page_count} total pages...\", file=log)\n",
    "            print(f\"Scraping final page of {page_count} total pages...\")\n",
    "        \n",
    "            post_data = scrape_one_page(data, browser)\n",
    "                            \n",
    "            print(\"Final page has been scraped.\", file=log)\n",
    "            print(\"Final page has been scraped.\")\n",
    "            last_page = True\n",
    "                       \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set \"what\" for the words to search, and \"where\" for the location search\n",
    "search_params = {\"what\": \"Data Analyst\", \"where\": \"United States\"}\n",
    "\n",
    "# Run the webscraper to collect all listings from all pages for the last 24 hours\n",
    "\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", t)\n",
    "\n",
    "print(f\"Beginning scraping process...{current_time}\", file=log)\n",
    "print(f\"Beginning scraping process...{current_time}\")\n",
    "\n",
    "try:\n",
    "    indeed_scraper(data, search_params, browser)\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", t)\n",
    "    print(f\"Scraping completed successfully! {current_time}\", file=log)\n",
    "    print(f\"Scraping completed successfully! {current_time}\")\n",
    "    \n",
    "    browser.quit()\n",
    "    print(\"Browser has been closed.\", file=log)\n",
    "    print(\"Browser has been closed.\")\n",
    "    \n",
    "except:\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", t)\n",
    "    print(f\"Something went wrong with the scraper.. :(\\n{current_time}\", file=log)\n",
    "    print(f\"Something went wrong with the scraper.. :(\\n{current_time}\")\n",
    "    \n",
    "    print(f\"Logs have been saved to {log_file}\", file=log)\n",
    "    print(f\"Logs have been saved to {log_file}\")\n",
    "    \n",
    "    browser.quit()\n",
    "    \n",
    "    print(\"Browser has been closed.\", file=log)\n",
    "    print(\"Browser has been closed.\")\n",
    "    log.close()\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f002d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name columns for putting the results into a DataFrame\n",
    "columns = [\"id\", \"title\", \"company\", \"location\", \"job_type\", \"salary\", \"time_recorded\", \"url\"]\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "listings_new_df = pd.DataFrame(data, columns = columns)\n",
    "#listings_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame as a .csv file\n",
    "try:\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", t)\n",
    "\n",
    "    file_path = '../data/listings_new.csv'\n",
    "\n",
    "    listings_new_df.to_csv(file_path, index=False)\n",
    "    print(f\"New results added and saved to file {file_path}\\n{current_time}\", file=log)\n",
    "    print(f\"New results added and saved to file {file_path}\\n{current_time}\")\n",
    "    \n",
    "except:\n",
    "    print(f\"Something went wrong saving the results to file.\", file=log)\n",
    "    print(f\"Something went wrong saving the results to file.\")\n",
    "    \n",
    "    print(f\"Logs have been saved to {log_file}\", file=log)\n",
    "    print(f\"Logs have been saved to {log_file}\")\n",
    "    \n",
    "    log.close()\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94debd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4861a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
