{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1043318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import splinter\n",
    "from splinter import Browser\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3da61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create browser object\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52db4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Indeed\n",
    "url = \"https://www.indeed.com/\"\n",
    "browser.visit(url)\n",
    "\n",
    "# Give the browser time to respond\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2debf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.com'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes the / from the end of the url\n",
    "url[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070f1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store results in\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c3c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all the posts on a single page\n",
    "def scrape_one_page(data, browser):\n",
    "    # Job Title\n",
    "    posts_in_page = browser.find_by_css(\".jobsearch-ResultsList\").find_by_css(\".job_seen_beacon\")\n",
    "        \n",
    "    for post in posts_in_page:\n",
    "        # Initialize 1D array to store this post's data\n",
    "        post_data = []\n",
    "            \n",
    "        # Grab job id\n",
    "        try:\n",
    "            job_id = post.find_by_css(\"a\").first[\"data-jk\"]\n",
    "            post_data.append(job_id)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "            \n",
    "        # Grab job title\n",
    "        try:\n",
    "            title = post.find_by_css(\"span\").first.text\n",
    "            post_data.append(title)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "            \n",
    "        # Grab company\n",
    "        try:\n",
    "            company = post.find_by_css(\".companyName\").first.text\n",
    "            post_data.append(company)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "            \n",
    "        # Grab location\n",
    "        try:\n",
    "            location = post.find_by_css(\".companyLocation\").first.text\n",
    "            post_data.append(location)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "            \n",
    "        # Grab job type (if available)\n",
    "        try:\n",
    "            job_type = post.find_by_css(\"[aria-label='Job type']\").first.find_by_xpath(\"..\").first.text\n",
    "            post_data.append(job_type)\n",
    "        except:\n",
    "            post_data.append(None)\n",
    "                \n",
    "        # Grab salary (if available)\n",
    "        try:\n",
    "            salary = post.find_by_css(\"[aria-label='Salary']\").first.find_by_xpath(\"..\").first.text\n",
    "            post_data.append(salary)\n",
    "        except:\n",
    "            try:\n",
    "                salary = post.find_by_css(\".estimated-salary\").first.find_by_css(\"span\").first.text\n",
    "                post_data.append(salary)\n",
    "            except:\n",
    "                post_data.append(None)\n",
    "            \n",
    "        # Grab job url\n",
    "        job_url = post.find_by_css(\"a\").first[\"href\"]\n",
    "        post_data.append(job_url)\n",
    "        \n",
    "        data.append(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facb59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all the posts on all pages\n",
    "def indeed_scraper(data, search_params, browser):\n",
    "    base_url = url[:-1]\n",
    "    # Find the what box & search the job title\n",
    "    what = browser.find_by_id(\"text-input-what\").first\n",
    "    what.fill(search_params[\"what\"])\n",
    "    \n",
    "    # Find the where box & search for the location\n",
    "    where = browser.find_by_id(\"text-input-where\").first\n",
    "    where.type(Keys.CONTROL + \"a\")\n",
    "    where.type(Keys.BACKSPACE)\n",
    "    where.fill(search_params[\"where\"])\n",
    "    \n",
    "    where.type(Keys.RETURN)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    date_posted = browser.find_by_text(\"Date posted\").first\n",
    "    date_posted.click()\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # This can be set to one of these options = [\"Last 24 Hours\", \"Last 3 days\", \"Last 7 days\", \"Last 14 days\"]\n",
    "    last_24_hours = browser.find_by_text(\"Last 24 hours\").first\n",
    "    last_24_hours.click()\n",
    "    \n",
    "    last_page = False\n",
    "    page_count = 0\n",
    "    \n",
    "    while not last_page:\n",
    "        try:  # This code runs for every page except the last page\n",
    "            next_page_button = browser.find_by_css(\"[aria-label='Next Page']\")\n",
    "            \n",
    "            page_count += 1\n",
    "            print(f\"Scraping page {page_count}...\")\n",
    "        \n",
    "            post_data = scrape_one_page(data, browser)\n",
    "                            \n",
    "            print(\"Clicking to next page.\")\n",
    "            next_page_button.click()\n",
    "                            \n",
    "        except:  # This code runs only for the last page\n",
    "            \n",
    "            page_count += 1\n",
    "            print(f\"Scraping final page of {page_count} total pages...\")\n",
    "        \n",
    "            post_data = scrape_one_page(data, browser)\n",
    "                            \n",
    "            print(\"Final page has been scraped.\")               \n",
    "            last_page = True\n",
    "                       \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d0a6ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning scraping process...\n",
      "Scraping page 1...\n",
      "Clicking to next page.\n",
      "Scraping page 2...\n",
      "Clicking to next page.\n",
      "Scraping page 3...\n",
      "Clicking to next page.\n",
      "Scraping page 4...\n",
      "Clicking to next page.\n",
      "Scraping page 5...\n",
      "Clicking to next page.\n",
      "Scraping final page of 6 total pages...\n",
      "Final page has been scraped.\n",
      "Scraping completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set \"what\" for the words to search, and \"where\" for the location search\n",
    "search_params = {\"what\": \"Data Scientist\", \"where\": \"United States\"}\n",
    "\n",
    "# Run the webscraper to collect all listings from all pages for the last 24 hours\n",
    "print(\"Beginning scraping process...\")\n",
    "try:\n",
    "    indeed_scraper(data, search_params, browser)\n",
    "    print(\"Scraping completed successfully!\")\n",
    "except:\n",
    "    print(\"Something went wrong with the scraper.. :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c85d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ../data/listings_all.csv\n",
      "Browser has been closed.\n"
     ]
    }
   ],
   "source": [
    "# Name columns for putting the results into a DataFrame\n",
    "columns = [\"id\", \"title\", \"company\", \"location\", \"job_type\", \"salary\", \"url\"]\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "listings_new_df = pd.DataFrame(data, columns = columns)\n",
    "listings_new_df\n",
    "\n",
    "# Open the .csv file which contains all the previously collected listings\n",
    "\n",
    "\n",
    "# Append the new listings to the existing ones\n",
    "\n",
    "\n",
    "\n",
    "# Save the updated DataFrame as a .csv file\n",
    "listings_new_df.to_csv(f'../data/listings_all.csv')\n",
    "print(\"File saved to ../data/listings_all.csv\")\n",
    "\n",
    "browser.quit()\n",
    "print(\"Browser has been closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4302fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
